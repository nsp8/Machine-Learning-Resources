{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfIHFMgeddUVq++VZ1GPlO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsp8/Machine-Learning-Resources/blob/colab-ml-practice/regression_flood_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ],
      "metadata": {
        "id": "AznAK__gqItw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zV5zdlXho9pT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patheffects import withStroke\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipython-autotime"
      ],
      "metadata": {
        "id": "N13PjnMVrcsb",
        "outputId": "ec94eb2e-af85-471f-9666-c138c69df061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autotime"
      ],
      "metadata": {
        "id": "t26Est30rfYU",
        "outputId": "788b89c4-1265-491c-a993-6fd2552c0af1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 476 Âµs (started: 2024-05-31 00:28:42 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Horizontal Chart for values associated with text labels"
      ],
      "metadata": {
        "id": "2flbAf3mp90H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Charts:\n",
        "    def __init__(self, data, width=18, height=9, font_family=\"Sans Serif\"):\n",
        "        self.fig, self.ax = plt.subplots(figsize=(width, height))\n",
        "        self.data = data\n",
        "        self.y_pos = [i * 0.9 for i in range(len(data))]\n",
        "        self.ax.barh(\n",
        "            self.y_pos,\n",
        "            self.data.values(),\n",
        "            height=0.55,\n",
        "            align=\"edge\",\n",
        "            color=\"blue\",\n",
        "        )\n",
        "        self.font_family = font_family\n",
        "\n",
        "    @property\n",
        "    def max_data_value(self):\n",
        "        return max(self.data.values())\n",
        "\n",
        "    @property\n",
        "    def min_data_value(self):\n",
        "        return min(self.data.values())\n",
        "\n",
        "    @property\n",
        "    def axis_period(self):\n",
        "        return int(self.max_data_value) // 10 or 1\n",
        "\n",
        "    def set_axes(self):\n",
        "        from math import ceil\n",
        "        data_size = len(self.data)\n",
        "        num_ticks = ceil(self.max_data_value)\n",
        "        self.ax.xaxis.set_ticks([i * self.axis_period for i in range(num_ticks)])\n",
        "        self.ax.xaxis.set_ticklabels(\n",
        "            [i * self.axis_period for i in range(0, num_ticks)],\n",
        "            size=16,\n",
        "            fontfamily=self.font_family,\n",
        "            fontweight=100,\n",
        "        )\n",
        "        self.ax.xaxis.set_tick_params(labelbottom=False, labeltop=True, length=0)\n",
        "        self.ax.set_axisbelow(True)\n",
        "        self.ax.grid(axis=\"x\", color=\"#8C92AC\", lw=1.2)\n",
        "        self.ax.spines[\"left\"].set_visible(False)\n",
        "        self.ax.spines[\"right\"].set_visible(False)\n",
        "        self.ax.spines[\"top\"].set_visible(False)\n",
        "        self.ax.spines[\"bottom\"].set_visible(False)\n",
        "        self.ax.spines[\"left\"].set_lw(1.5)\n",
        "        self.ax.spines[\"left\"].set_capstyle(\"butt\")\n",
        "        self.ax.yaxis.set_visible(False)\n",
        "        return self\n",
        "\n",
        "    def add_bar_text(self, label_object: dict):\n",
        "        self.ax.text(\n",
        "            x=label_object[\"x_pos\"],\n",
        "            y=label_object[\"y_pos\"],\n",
        "            s=label_object[\"text\"],\n",
        "            c=label_object[\"color\"],\n",
        "            fontfamily=self.font_family,\n",
        "            fontsize=12,\n",
        "            va=\"center\",\n",
        "            path_effects=label_object[\"path_effects\"],\n",
        "        )\n",
        "\n",
        "    def add_labels(self, padding: float = 0.3):\n",
        "        offset = 0.04\n",
        "        for label, value, y_pos in zip(\n",
        "            self.data.keys(), self.data.values(), self.y_pos\n",
        "        ):\n",
        "            metric_label = dict(\n",
        "                x_pos=int(self.min_data_value) + (padding * 2.2),\n",
        "                y_pos=(y_pos + 0.25),\n",
        "                text=label,\n",
        "                color=\"white\",\n",
        "                path_effects=[withStroke(linewidth=6, foreground=\"green\")]\n",
        "            )\n",
        "            value_label = dict(\n",
        "                x_pos=value,\n",
        "                y_pos=(y_pos + 0.25),\n",
        "                text=f\"{round(value, 3)}0\",\n",
        "                color=\"green\",\n",
        "                path_effects=[withStroke(linewidth=6, foreground=\"white\")]\n",
        "            )\n",
        "            if value < 0:\n",
        "                metric_label[\"x_pos\"] = self.min_data_value - (padding * 0.7) - offset\n",
        "                metric_label[\"path_effects\"] = [withStroke(linewidth=6, foreground=\"red\")]\n",
        "                metric_label[\"color\"] = \"white\"\n",
        "                value_label[\"color\"] = \"red\"\n",
        "                value_label[\"x_pos\"] -= offset\n",
        "            self.add_bar_text(metric_label)\n",
        "            self.add_bar_text(value_label)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def add_title(self, title, pos=0.5):\n",
        "        self.fig.text(\n",
        "            pos, 0.925, title, fontsize=16, fontweight=\"bold\", fontfamily=self.font_family\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def plot(self, title=None):\n",
        "        self.set_axes().add_labels()\n",
        "        if title:\n",
        "            self.add_title(title)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "lXlscVpghcea",
        "outputId": "39e6cb87-65f5-4b62-9963-8ae043512766",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 10.8 ms (started: 2024-05-31 00:28:42 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JgqtA8S2TZB",
        "outputId": "01f741a6-0a05-471f-d656-4bba1467e0fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive/; to attempt to forcibly remount, call drive.mount(\"/gdrive/\", force_remount=True).\n",
            "time: 879 ms (started: 2024-05-31 00:28:42 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Loading**"
      ],
      "metadata": {
        "id": "_NzB-dVoqkM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_folder_path = os.path.join('/gdrive', 'My Drive', 'Projects', 'ML Practice', 'Regression: flood prediction', 'data')\n",
        "if os.path.exists(base_folder_path):\n",
        "    print(\"Loading data ...\")\n",
        "    train_data = pd.read_csv(os.path.join(base_folder_path, \"train.csv\"))\n",
        "    test_data = pd.read_csv(os.path.join(base_folder_path, \"test.csv\"))\n",
        "    print(\"Loaded train and test files from Drive\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"Could not locate files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAsh3v_Bp7IA",
        "outputId": "6187ebb0-2d30-466a-96fc-56705dbde948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Exploration**"
      ],
      "metadata": {
        "id": "mkRbZ6R1qVWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic statistical analyses"
      ],
      "metadata": {
        "id": "MHwIbcCHqc1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.describe()"
      ],
      "metadata": {
        "id": "8Kv4mG4AszAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Null counts\n",
        "train_data.isnull().sum()"
      ],
      "metadata": {
        "id": "QYHFUQw8tECy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data type information\n",
        "train_data.info()"
      ],
      "metadata": {
        "id": "-BWCTTO2vKls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layout of data\n",
        "features_train = train_data.set_index(\"id\")\n",
        "target_column = \"FloodProbability\"\n",
        "label_train = features_train.pop(target_column)\n",
        "features_train.hist(bins=15, figsize=(20, 45), layout=(4, 5))"
      ],
      "metadata": {
        "id": "047aiCq-yIrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardizing data\n",
        "\n",
        "def standardize(data):\n",
        "    standard_scaler = StandardScaler()\n",
        "    standard_scaler.fit(data)\n",
        "    return standard_scaler.transform(data)\n",
        "    # _data = data.copy(deep=True)\n",
        "    # return (_data - _data.mean(axis=0)) / _data.std(axis=0)\n",
        "\n",
        "\n",
        "features_train_standardized = pd.DataFrame(standardize(features_train), columns=features_train.columns)\n",
        "features_train_standardized.hist(bins=15, figsize=(20, 45), layout=(4, 5))\n",
        "# print(features_train_standardized.shape)"
      ],
      "metadata": {
        "id": "Nhz2gfdWzl_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_labels(label_data):\n",
        "    mean, std = label_data.mean(), label_data.std()\n",
        "    return label_data.apply(lambda v: (v - mean) / std)\n",
        "\n",
        "\n",
        "label_train_standardized = pd.DataFrame(standardize_labels(label_train), columns=[target_column])\n",
        "label_train_standardized.hist(bins=15, figsize=(5, 10))"
      ],
      "metadata": {
        "id": "8ErssM4j7dHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train test split"
      ],
      "metadata": {
        "id": "iADjfuF-rWWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features_train_standardized, label_train_standardized, test_size=0.25)"
      ],
      "metadata": {
        "id": "0EMipjp68VZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.columns"
      ],
      "metadata": {
        "id": "Gj_Jrm8mB3o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correlation matrix of training features"
      ],
      "metadata": {
        "id": "UF5pOfd8KN7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "sns.heatmap(X_train.corr(), vmax=1, square=True, annot=True, cmap=\"plasma\")"
      ],
      "metadata": {
        "id": "JOMZEZUcGi6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Analyses"
      ],
      "metadata": {
        "id": "kVDaCl4rrlyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureAnalyzer:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.pca = PCA()\n",
        "        self.pca.fit_transform(self.X)\n",
        "\n",
        "    @property\n",
        "    def X_pca(self):\n",
        "        x_pca = self.pca.fit_transform(self.X)\n",
        "        return pd.DataFrame(x_pca, columns=[f\"PC{i+1}\" for i in range(x_pca.shape[1])])\n",
        "\n",
        "    @property\n",
        "    def pca_loadings(self):\n",
        "        return pd.DataFrame(self.pca.components_.T, columns=self.X_pca.columns, index=self.X.columns)\n",
        "\n",
        "    def plot_variance(self, width=8, dpi=100):\n",
        "        fig, axes = plt.subplots(1, 2)\n",
        "        n = self.pca.n_components_\n",
        "        print(f\"\\n{'-' * 35}\\nPCA Loadings \\n{'-' * 35}\\n\")\n",
        "        print(self.pca_loadings)\n",
        "        grid = np.arange(1, n+1)\n",
        "        explained_variance = self.pca.explained_variance_ratio_\n",
        "        axes[0].bar(grid, explained_variance)\n",
        "        axes[0].set(xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0))\n",
        "        cumulative_variance = np.cumsum(explained_variance)\n",
        "        axes[1].plot(np.r_[0, grid], np.r_[0, cumulative_variance], marker=\"h\")\n",
        "        axes[1].set(xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0))\n",
        "        fig.set(figwidth=width, dpi=dpi)\n",
        "        return axes\n",
        "\n",
        "    def make_mi_scores(self, discrete_features: bool = False):\n",
        "        mi_regression = mutual_info_regression(self.X, self.y, discrete_features=discrete_features)\n",
        "        return pd.Series(mi_regression, name=\"Mutual Info Scores\", index=self.X.columns).sort_values(ascending=False)\n",
        "\n",
        "    def check_outliers(self):\n",
        "        sns.catplot(\n",
        "            y=\"value\",\n",
        "            col=\"variable\",\n",
        "            data=self.X_pca.melt(),\n",
        "            kind=\"boxen\",\n",
        "            sharey=False,\n",
        "            col_wrap=2\n",
        "        )\n",
        "\n",
        "\n",
        "def show_feature_analysis_artifacts(X, y):\n",
        "    feature_analyzer = FeatureAnalyzer(X, y)\n",
        "    feature_analyzer.plot_variance()\n",
        "    mi_scores = feature_analyzer.make_mi_scores()\n",
        "    feature_analyzer.check_outliers()\n",
        "    print(f\"\\n{'-' * 35}\\nMutual Information Scores \\n{'-' * 35}\\n\")\n",
        "    print(mi_scores)\n",
        "    return feature_analyzer\n",
        "\n",
        "\n",
        "feature_analyzer_object = show_feature_analysis_artifacts(X_train, y_train)"
      ],
      "metadata": {
        "id": "YbbF3rFvhVI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_analyzer_object.pca_loadings"
      ],
      "metadata": {
        "id": "Q5E4ILNJumIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize principal components"
      ],
      "metadata": {
        "id": "_6_VCEH3wx0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_principal_component(feature_analyzer_object, level=1):\n",
        "    if level < 1:\n",
        "        raise ValueError(\"PC level can't be less than 1\")\n",
        "    _loadings = feature_analyzer_object.pca_loadings\n",
        "    _column = _loadings.columns[level - 1]\n",
        "    pc_dataframe = _loadings.loc[:, _column].reset_index().to_dict(orient=\"records\")\n",
        "    feature_to_pc1 = {_entry[\"index\"]: float(_entry[_column]) for _entry in pc_dataframe}\n",
        "    chart = Charts(data=feature_to_pc1, font_family=\"monospace\")\n",
        "    chart.plot(title=_column)\n",
        "\n",
        "plot_principal_component(feature_analyzer_object, level=1)"
      ],
      "metadata": {
        "id": "VfD-Ca7ht7xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adding new features [test]"
      ],
      "metadata": {
        "id": "7m5NqP4nZYm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_new = X_train.copy(deep=True)\n",
        "X_train_new[\"IsCoastalVulnerability\"] = X_train[\"CoastalVulnerability\"] > 0\n",
        "X_train_new[\"IsIneffectiveDisasterPreparedness\"] = X_train[\"IneffectiveDisasterPreparedness\"] > 0\n",
        "X_train_new[\"IsDeforestation\"] = X_train[\"Deforestation\"] > 0\n",
        "X_test_new = X_test.copy(deep=True)\n",
        "X_test_new[\"IsCoastalVulnerability\"] = X_test[\"CoastalVulnerability\"] > 0\n",
        "X_test_new[\"IsIneffectiveDisasterPreparedness\"] = X_test[\"IneffectiveDisasterPreparedness\"] > 0\n",
        "X_test_new[\"IsDeforestation\"] = X_test[\"Deforestation\"] > 0"
      ],
      "metadata": {
        "id": "1vC5H06ffadT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training the model: XGBoost Regressor**"
      ],
      "metadata": {
        "id": "J0Bs2nm2fcA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import learning_curve, validation_curve\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "class RegressionModel:\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_data: tuple,\n",
        "        test_data: tuple,\n",
        "        learning_rate=0.01,\n",
        "        n_estimators=1000,\n",
        "        early_stopping_rounds=5,\n",
        "        corss_validation_folds=10,\n",
        "        n_jobs=1\n",
        "    ):\n",
        "        self.model = XGBRegressor()\n",
        "        self.train_data = train_data\n",
        "        self.test_data = test_data\n",
        "        self.learning_curve_params = {\n",
        "            \"X\": train_data[0],\n",
        "            \"y\": train_data[1],\n",
        "            \"train_sizes\": np.linspace(0.1, 1, 10),\n",
        "            \"cv\": corss_validation_folds,\n",
        "            \"n_jobs\": n_jobs\n",
        "        }\n",
        "        self.validation_curve_params = {\n",
        "            \"X\": train_data[0],\n",
        "            \"y\": train_data[1],\n",
        "            \"cv\": corss_validation_folds,\n",
        "            \"param_name\": \"xgbregressor__learning_rate\",\n",
        "            \"param_range\": np.linspace(0.0001, 0.1, 100),\n",
        "            \"n_jobs\": n_jobs\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def pipeline(self):\n",
        "        return make_pipeline(self.model)\n",
        "\n",
        "    def plot_learning_curve(self, curve_function=\"learning_curve\"):\n",
        "        def plot(data_sizes, scores, plot_params):\n",
        "            data_average = np.mean(scores, axis=1)\n",
        "            data_std = np.std(scores, axis=1)\n",
        "            plt.plot(\n",
        "                data_sizes,\n",
        "                data_average,\n",
        "                **plot_params\n",
        "            )\n",
        "            plt.fill_between(\n",
        "                data_sizes,\n",
        "                data_average + data_std,\n",
        "                data_average - data_std,\n",
        "                alpha=0.15,\n",
        "                color=plot_params[\"color\"]\n",
        "            )\n",
        "        if curve_function == \"learning_curve\":\n",
        "            train_sizes, train_scores, test_scores = learning_curve(\n",
        "                estimator=self.pipeline,\n",
        "                **self.learning_curve_params\n",
        "            )\n",
        "            plt.xlabel(\"Number of training examples\")\n",
        "        else:\n",
        "            train_scores, test_scores = validation_curve(\n",
        "                estimator=self.pipeline,\n",
        "                **self.validation_curve_params\n",
        "            )\n",
        "            train_sizes = self.validation_curve_params[\"param_range\"]\n",
        "            plt.xlabel(\"XGBRegressor learning rate\")\n",
        "        plot(train_sizes, train_scores, dict(color=\"blue\", marker=\"o\", markersize=5, label=\"Training accuracy\"))\n",
        "        plot(train_sizes, test_scores, dict(color=\"red\", marker=\"s\", linestyle=\"--\", markersize=5, label=\"Validation accuracy\"))\n",
        "        plt.grid()\n",
        "\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.legend(loc=\"upper right\")\n",
        "        # plt.ylim([0.8, 1.03])\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "2CXV0aT1ffV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = list()"
      ],
      "metadata": {
        "id": "8UW36qX750VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RegressionModel(train_data=(X_train, y_train), test_data=(X_test, y_test))\n",
        "model.plot_learning_curve(learning_curve)\n",
        "model.plot_learning_curve(validation_curve)\n",
        "models.append(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEM7f5NI518G",
        "outputId": "ec214b6b-a62e-4fad-9941-5cc762a2657a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored on calling ctypes callback function: <bound method DataIter._next_wrapper of <xgboost.data.SingleBatchInternalIter object at 0x78646be37040>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 589, in _next_wrapper\n",
            "    def _next_wrapper(self, this: None) -> int:  # pylint: disable=unused-argument\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = RegressionModel(train_data=(X_train_new, y_train), test_data=(X_test_new, y_test))\n",
        "model.plot_learning_curve(learning_curve)\n",
        "model.plot_learning_curve(validation_curve)\n",
        "models.append(model)"
      ],
      "metadata": {
        "id": "rm4CiD2Tb-Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adding features based on the PCA"
      ],
      "metadata": {
        "id": "5oHUxcIYMC-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pca = feature_analyzer_object.pca.fit_transform(X_train)\n",
        "X_test_pca = feature_analyzer_object.pca.fit_transform(X_test)\n",
        "model = RegressionModel(train_data=(X_train_pca, y_train), test_data=(X_test_pca, y_test))\n",
        "model.plot_learning_curve(learning_curve)\n",
        "model.plot_learning_curve(validation_curve)\n",
        "models.append(model)"
      ],
      "metadata": {
        "id": "ZhYFpZR8DZLY",
        "outputId": "f6afe710-c075-4aba-9fed-289ee19bda20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'feature_analyzer_object' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7772c00b9184>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_analyzer_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_analyzer_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegressionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_pca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_pca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_curve\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_curve\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'feature_analyzer_object' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pca = feature_analyzer_object.pca.fit_transform(X_train)\n",
        "X_test_pca = feature_analyzer_object.pca.transform(X_test)\n",
        "model = RegressionModel(train_data=(X_train_pca, y_train), test_data=(X_test_pca, y_test))\n",
        "model.plot_learning_curve(learning_curve)\n",
        "model.plot_learning_curve(validation_curve)\n",
        "models.append(model)"
      ],
      "metadata": {
        "id": "SZ_yKSXwMHP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import HalvingRandomSearchCV\n",
        "\n",
        "\n",
        "xgbr_param_grid = [\n",
        "    {\n",
        "        \"xgbr__learning_rate\": np.linspace(0.0001, 0.1, 100)\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "wzMYNAeXqddO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}